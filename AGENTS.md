# Repository Guidelines

## Project Structure & Module Organization
- `debatebench/` -- CLI entrypoint plus core modules: config, debate orchestration, judge aggregation, Elo ratings, storage, settings/schema. Add providers by extending adapters in `models.py`.
- `configs/` -- user templates (`config.yaml`, `topics.json`, `models.yaml`, `judges.yaml`). Generated by `debatebench init` if missing.
- `results/` -- debates (`debates_*.jsonl`), ratings, CSV summaries, and plots (`viz_*`, `plots_*`). Treat as artifacts; don't commit large runs.
- `notebooks/` -- exploratory analysis; keep outputs light.
- `scripts/` -- placeholder for helpers; prefer reusable code in the package.

## Build, Test, and Development Commands
- Install editable: `pip install -e .`
- Bootstrap configs/results: `debatebench init`
- Run debates: `debatebench run --sample-topics 3 --debates-per-pair 1 --run-tag demo` (topic picker first; OpenRouter picker defaults all models OFF; shows text-in/text-out models only; sorted alphabetically; arrow keys move, Enter/Space toggles on/off, c continues; judges default to the selected models, or use `--no-judges-from-selection` for a separate judge picker; debater/judge max tokens default 512/256 with automatic downshift on 402 errors; at least two judges required; writes `results/debates_demo.jsonl`, then summarizes and plots).
- Recompute ratings: `debatebench rate --debates-path results/debates.jsonl`
- Inspect artifacts: `debatebench show-leaderboard --top 10`; `debatebench inspect-debate <uuid>`
- Re-run summaries/plots: `debatebench summarize --debates-path results/debates_demo.jsonl --out-dir results/viz_demo`; `debatebench plot --viz-dir results/viz_demo --out-dir results/plots_demo`

## Coding Style & Naming Conventions
- Python 3.9+; Pydantic v1 models; Typer CLI with Rich output. Use 4-space indentation and type hints.
- Keep randomness seeded via parameters (`seed`, `run_tag`) for reproducibility.
- File/module names snake_case; config IDs short, lowercase strings (e.g., `gpt4o`, `claude-sonnet`).
- When adding adapters, mirror the OpenAI/HTTP patterns in `models.py` and expose tunables through `parameters` in YAML.

## Testing Guidelines
- No automated tests yet; prefer `pytest` under `tests/` with fixtures that load small sample debates from `results/` or synthetic `DebateRecord` objects.
- Seed RNGs in tests to keep Elo and judge outcomes deterministic.
- For CLI flows, use Typer's testing utilities or invoke commands via `python -m debatebench.cli ...` against temp dirs.

## Commit & Pull Request Guidelines
- Match history style: short imperative sentences, no trailing period (e.g., "Add auto-tagged runs", "Update judge parsing").
- Group related changes (code + docs + sample configs). Avoid committing large result files or secrets.
- PRs: include a brief summary, commands/tests run, and links to key plots/CSVs. Call out config/schema changes so reviewers can regenerate templates.

## Security & Configuration Notes
- Secrets live in `.env` (`OPENAI_API_KEY`, optional `HTTP_BEARER_TOKEN`). Never commit `.env` or embed keys in configs/notebooks.
- OpenRouter: set `OPENROUTER_API_KEY` (plus optional `OPENROUTER_SITE_URL`/`OPENROUTER_SITE_NAME` for referral headers). Use `provider: openrouter` in `models.yaml`; endpoint defaults to `https://openrouter.ai/api/v1/chat/completions` unless overridden.
- Endpoints come from `models.yaml`; prefer HTTPS, set reasonable timeouts, and avoid logging full prompts/responses if they may contain sensitive data.
