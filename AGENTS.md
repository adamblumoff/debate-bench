# Repository Guidelines

## Project Structure & Module Organization
- `debatebench/` -- CLI entrypoint plus core modules: config, debate orchestration, judge aggregation, Elo ratings, storage, settings/schema. Add providers by extending adapters in `models.py`.
- `configs/` -- user templates (`config.yaml`, `topics.json`, `models.yaml`, `judges.yaml`). Generated by `debatebench init` if missing.
- `results/` -- debates (`debates_*.jsonl`), ratings, CSV summaries, and plots (`viz_*`, `plots_*`). Treat as artifacts; don't commit large runs.
- `notebooks/` -- exploratory analysis; keep outputs light.
- `scripts/` -- placeholder for helpers; prefer reusable code in the package.

## Build, Test, and Development Commands
- Install editable: `pip install -e .`
- Bootstrap configs/results: `debatebench init`
- Run debates: `debatebench run --sample-topics 3 --debates-per-pair 1 --run-tag demo` (topic picker + OpenRouter picker; seed defaults to 12345; `high_tokens` defaults to 3200/2200/1400; judges can come from the debater pool with `--judges-from-selection` and the active debaters are excluded from judge sampling; `--dry-run` shows live-priced cost estimates and saves `dryrun_schedule.json`; `--resume` skips completed debates; summaries/plots/ratings auto-run). Useful toggles: `--no-tui-wizard`, `--no-topic-select`, `--no-openrouter-select`, `--skip-on-empty`.
- Recompute ratings: `debatebench rate --debates-path results/debates.jsonl`
- Inspect artifacts: `debatebench show-leaderboard --top 10`; `debatebench inspect-debate <uuid>`
- Re-run summaries/plots: `debatebench summarize --debates-path results/debates_demo.jsonl --out-dir results/viz_demo`; `debatebench plot --viz-dir results/viz_demo --out-dir results/plots_demo`

## Coding Style & Naming Conventions
- Python 3.9+; Pydantic v1 models; Typer CLI with Rich output. Use 4-space indentation and type hints.
- Keep randomness seeded via parameters (`seed`, `run_tag`) for reproducibility (default seed=12345).
- File/module names snake_case; config IDs short, lowercase strings (e.g., `gpt4o`, `claude-sonnet`).
- When adding adapters, mirror the OpenRouter HTTP patterns in `models.py` and expose tunables through `parameters` in YAML.

## Testing Guidelines
- No automated tests yet; prefer `pytest` under `tests/` with fixtures that load small sample debates from `results/` or synthetic `DebateRecord` objects.
- Seed RNGs in tests to keep Elo and judge outcomes deterministic.
- For CLI flows, use Typer's testing utilities or invoke commands via `python -m debatebench.cli ...` against temp dirs.

## Commit & Pull Request Guidelines
- Match history style: short imperative sentences, no trailing period (e.g., "Add auto-tagged runs", "Update judge parsing").
- Group related changes (code + docs + sample configs). Avoid committing large result files or secrets.
- PRs: include a brief summary, commands/tests run, and links to key plots/CSVs. Call out config/schema changes so reviewers can regenerate templates.

## Security & Configuration Notes
- Secrets live in `.env` (`OPENROUTER_API_KEY`, optional `OPENROUTER_SITE_URL`/`OPENROUTER_SITE_NAME`). Never commit `.env` or embed keys in configs/notebooks.
- Use `provider: openrouter` in `models.yaml`/`judges.yaml`; endpoint defaults to `https://openrouter.ai/api/v1/chat/completions` unless overridden.
- Endpoints come from `models.yaml`; prefer HTTPS, set reasonable timeouts, and avoid logging full prompts/responses if they may contain sensitive data.
